---
title: "Week 11 Slides"
subtitle: "Review + Cluster Analysis"
author: "Andrew Katz"
institute: "Department of Engineering Education <br><br> Virginia Tech"
date: "2021-03-30"
output:
  xaringan::moon_reader:
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---
```{r setup-chunk, echo = FALSE, message = FALSE}
library(tidyverse)
library(car)


xaringanExtra::use_panelset()


```


---

.center[

# Week 11 Announcements

]

--
### I am working on catching up with your reading reflections

--

### I have commented on everyone's final project pitch.

* If you have questions about what I wrote, please let me know!


--

### Don't forget about the feedback surveys


---

.center[
# Week 11 Announcements
]

## New Schedule!

* March 30 - Review + Start Cluster Analysis

* April 6 - No class

* April 13 - Cluster analysis + Reporting results

* April 20 - Where to go next - Factor Analysis, Multilevel Models, Bayesian Models, Causal Inference

* April 27 - Presentation Day 1

* May 4 - Presentation Day 2 + Wrap up

--

#### Let's talk about presentation scheduling

---

.center[
# Week 11 Reading Assignment

### Links for clustering

#### Mentzer and Jackson 2017

### Video options


]



---


# Today's Plan

--

### Ethics week :)

--

### Review basics ideas in general(ized) linear models

--

### Review nonparametric tests

--

### Review Chi Square test of association



--

### _Brief_ Intro to Cluster Analysis

--

### Work time for problem sets and final project

---


class: inverse, middle, center



# Review




---

# General Modeling Philosophy

--

The general approach for the next several weeks is to model the outcome variable as a function of some predictor(s) plus an error term. 

--

Mathematically, this looks like:

.center[
$outcome_i = model + error_i$
]

where the $i$ subscript refers to the $i^{th}$ person in the sample. 



---

# The Simple Regression Model

--

In simple regression, the model looks like:

.center[
$Y_i = (b_0 + b_1 * X_i) + \epsilon_i$

]

--

* The $Y_i$ is the value of the outcome variable for person $i$

--

* $b_0$ and $b_1$ are the regression coefficients

  * These are the things we are trying to find
  
  * They are the same for each person

--
  
* The $X_i$ is the values of the predictor variable for person $i$

--

* The $\epsilon_i$ is the error term for person $i$

  * This accounts for the discrepancy between what the model predicts person $i$'s outcome variable will be and what it _actually_ is observed to be



---

# Finding Coefficient Values

--

* To find coefficient values, we use the method of least squares

--

* The basic philosophy is to find values for $b_0$ and $b_1$ that minimize the distance between the actual observed outcome, $y_i$, and the estimated outcome, $\hat{y}_i$

--

* As a reminder, $\hat{y}_i$ is calculated as:  $\hat{y}_i = b_0 + b_1*X_i$ 

--

* In a _frequentist_ paradigm this is done using maximum likelihood estimation
  * The math for this is beyond the scope of this class

--

* Practically speaking, we use this kind of syntax in R: 
.center[
`lm(outcome ~ predictor, data = your_data)`
]


---

# Interpreting Coefficient Values

.pull-left[

* The intercept $b_0$ is the value of the outcome when the value of the predictor is 0

* A common strategy is to first center the $x_i$ values by subtracting $\overline{x}$ from each $x_i$

* If you center your variables first, then the interpretation of $b_0$ is the value of the outcome at an average value of the predictor

]

--

.pull-right[

* The slope $b_1$ is the change in the outcome variable for a 1-unit change in the predictor variable

]


---

# What Makes a Good Model?

--

* Short answer: Small sum of squares. Specifically, we want to minimize the squared distance between $y_i$ and $\hat{y}_i$

--

* In the book this is noted as $deviation = \sum_{i=1}^{N}(observed_i - estimated_i)^2$

--

* Longer answer: Think about three kinds of sums of squares:

  1. Sum of squares total $SS_T$
  2. Sum of squares residual $SS_R$
  3. Sum of square model $SS_M$

--

* We are interested in reducing $SS_R$




---
# Good Models (cont.)

* Another way of saying this: we want to explain as much of the variation in the data as possible

--

* The total variation is $SS_T$

--

* The model variation is $SS_M$

--

* If $SS_T$ and $SS_M$ are close to each other, then their ratio is close to 1

--

* This is how we calculate $R^2$:
.center[
$R^2 = \frac{SS_M}{SS_T}$
]


--

* Note that $SS_T = SS_M + SS_R$, which is a fancy way of saying the total variance can be decomposed into explained + unexplained variance (where "explained" variance refers to the variance explained by the model)

--

* If we take the _mean sum of squares_, we get $MS_R$ and $MS_M$, which are used to calculate the F statistic:

.center[
$F = \frac{MS_R}{MS_M}$
]




---

# Interpreting measures of fit

--

* A higher $R^2$ value corresponds to a better fit

--

* Look at the equation for $R^2$:

.center[
$R^2 = \frac{SS_M}{SS_T} = \frac{SS_T - SS_R}{SS_T} = 1 - \frac{SS_R}{SS_T}$
]

--

* Takeaway: a smaller residual sum of squares corresponds to a higher $R^2$ value


---

# Assessing Individual Predictors in a Model

--

* The null model in regression is for each predictor to equal 0

--

* Our hypothesis testing is testing against this null hypothesis that $b_1 = 0$

--

* This means the alternative hypothesis is $b_1 \neq 0$

--

* We test whether we have good reason to reject the null hypothesis that $b_1 = 0$ by calculating the t-statistic:
.center[
$t = \frac{b_{1, model} - b_{1, null}}{SE_{b_1}}$
]

--

* This simplifies to 
.center[
$t = \frac{b_{1, model}}{SE_{b_1}}$
]


---
class: center, inverse, middle

# Regression: The Sequel

---



# Multiple regression basics

--

* We now have a model with multiple predictors. Mathematically, this looks like:

.center[
$Y_i = (b_0 + b_1 * X_{1i} + b_2 * X_{2i} + \dots + b_n * X_{ni}) + \epsilon_i$

]

--

* We still have the same considerations for sums of squares, e.g., $SS_T$, and trying to minimize $SS_R$

--

* Another way to think about this is that we are trying to explain as much variance in the outcome as possible

--

* As with simple regression, $R^2$ is the metric we will use to tell us how well our model fits
  * Except we pay attention to the adjusted $R^2$ which penalizes more complex models

---

# Additional measures of fit

--

### Akaike Information Criteria (AIC)
.center[
$AIC = n  \ln{\frac{SSE}{n}} + 2k$
]

  * We are trying to reduce AIC

  * There are no general rules of thumb for "good" or "bad" fit



---





---

# Checking assumptions

--

* Continuous or categorical predictors

--

* Non-zero variance

--

* No perfect multicollinearity
  * Variance Inflation Factor with `car::vif()`

--

* Predictors uncorrelated with external variables

--

* Homoskedasticity 

--

* Independent errors
  * Durbin Watson test with `car::dwt()`
--

* Normally distributed errors
  * Plot fitted values vs residuals

--

* Independence
  

--

* Linearity


---

# Model cross validation

--

.pull-left[

* Adjusted $R^2$
]

--

.pull-right[
* Data splitting

]


---

# Outliers and influential cases

.pull-left[
### Outliers

* residuals `resid()`

* standardized residuals `rstandard()`

* studentized residuals `rstudent()`

]

.pull-right[
### Influential Cases

* Cook's distance with `cooks.distance`

* DFBeta with `dfbeta()`

* DFFit with `dffits()`

* hatvalues (leverage) with `hatvalues()`

* covariance ratio `covratio()`

]


---
# False positives and false negatives


---
# More on p values


---
# More on confidence intervals


---

class: center, inverse, middle

# Nonparametric tests


---


---


---


class: center, inverse, middle

# Chi Square Test of Association

---

# Contingency tables and the test statistic


---



---

class: center, inverse, middle

# Cluster Analysis


---

# What is the point?

--

* Grouping data points that are similar

--

* Data reduction for observations (as opposed to reduction of variables - that's factor analysis)

---

# Types of clustering

* Centroid vs density-based

* Flat vs hierarchical


---

# Step 1

--

## Feature selection

---

# Step 2

## Compute similarities

* Euclidean distance
--

* Manhattan distance

--

* Pearson correlation distance

--

### This produces a distance matrix

---

# Step 3

## Perform the actual clustering


---

# Design Choices

* Type of clustering algorithm
  * K-means
  * Gaussian mixture model
  * DBSCAN
  * Hierarchical
  * Ward
  * HDBSCAN

* Type of clustering rule
  * Single linkage
  * Complete linkage
  * Average linkage
  