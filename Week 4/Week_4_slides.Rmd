---
title: "Week 4 Slides"
subtitle: "Assumptions and Correlations"
author: "Andrew Katz"
institute: "Department of Engineering Education <br><br> Virginia Tech"
date: "2021-02-09"
output:
  xaringan::moon_reader:
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---
```{r setup-chunk, echo = FALSE, message = FALSE}
library(tidyverse)
library(car)


xaringanExtra::use_panelset()


```


---

.center[

# Week 4 Announcements

]

--
### Problem Set 1 is due next week :)

* Let's take a minute to review it now...

--

### I have started compiling course notes in a book available at this URL: https://andrewskatz.github.io/test_course_note/

--

### If you have suggestions about the class, the feedback surveys are a great option

--

### I am going to continue posting resources that I think you all might find helpful in the #resources channel in the course Slack workspace

--

### I will update the key terms list this week




---

.center[
# Week 4 Reading Assignment
]

.pull-left[
## DSUR Chapter 5  
### Assumptions
]

.pull-right[
## DSUR Chapter 6
### Correlations

]


---


# Today's Plan

### Review Chapter 5 of DSUR

--

### Review Chapter 6 of DSUR

--

### Team practice in R

---


class: inverse, middle, center



# Chapter Five

### Assumptions


---

# Four Big Assumptions

--

1. Normality of the data

--

2. Homogeneity of variance

--

3. Interval data

--

4. Independence





---

# Normal Distribution of the Data

--

* Central limit theorem - the sampling distribution (distribution of the sample means) is normally distributed for large sample sizes. This gives us the 30 sample size rule of thumb.
--

* A reminder: 
$\lim_{n\to\infty} \frac{\overline{X_n} - \mu}{\frac{\sigma}{\sqrt{n}}} \sim \mathcal{N} (0, \,1)$ 

--
* Note that $\frac{\sigma}{\sqrt{n}}$ is the standard error (which is the standard deviation of the sampling distribution of the sample mean). This is the same thing that we do when we need to center a random variable by first subtracting its mean and then dividing by its standard deviation: $\frac{x_i - \overline{x}}{s}$

--

* Standard error: standard deviation of the sampling distribution (we saw this last week)
--

* _There is a difference in having a normal distribution and a normal sampling distribution_

---

# Normal Distribution (cont.)

--

* Shapiro-Wilk test can determine if data are normally distributed. Use `shapiro.test()`
--

* We can use histograms and Q-Q plots to see if data are normally distributed (this is for visual inspection)
--

* With large sample sizes, you can get statistically significant result from the Shapiro-Wilk test, so it is good to also look at histograms and Q-Q plots to check for normality

---

# Examples of Normality

.panelset[

.panel[.panel-name[Normal]

.pull-left[
```{r norm-data}
x_norm <- rnorm(n = 100, mean = 10, sd = 3)
```

```{r out.width = '300px', out.height = '300px'}
hist(x_norm)

```

]

.pull-right[
```{r out.width = '300px', out.height = '300px'}
qqnorm(x_norm)

```

]

]

.panel[.panel-name[Non-normal]

.pull-left[
```{r non-norm-data}
x_non_norm <- rpois(n = 100, lambda = 8)
```

```{r out.width = '300px', out.height = '300px'}
hist(x_non_norm)

```

]

.pull-right[
```{r out.width = '300px', out.height = '300px'}
qqnorm(x_non_norm)

```

]

]

]


---


# Homogeneity of Variance

* Variances of a variable should be the same across different groups

--

* The variance of one variable should be the same at all levels of the other variable

--

* Variance of your outcome variable should be the same in each group 

--

* Spread within samples is the same

--

* The idea behind homogeneity of variance is that these standard deviations should be close together - the box plots should be similar

---

# Homogeneity of Variance (cont.)

--

* When box plots are pretty different, you can tell that the assumption of homogeneity of variance

--

* Heterogeneity of variance - some groups have huge variance while others have small variance

--

* Hartley’s F_max can test homogeneity of variance (not common)

--

* Levene’s test is commonly used to test homogeneity of variance
  * Null hypothesis - the variance is homogeneous (same across groups)
  * Rejecting null hypothesis means the variance is not homogeneous
  * Commonly used with T-tests and ANOVA

---

# Homogeneity of Variance Example


.panelset[

.panel[.panel-name[Setup]


* Example: Let's say we are interested in knowing whether there is a difference among disciplines in how much students in each discipline study on average every week. 



* We go out and sample 50 civil engineering students and 50 chemical engineering students. We are looking at the number of minutes spent on homework every day.



* Suppose $\overline{x}_{civ} = 80$ minutes and $\overline{x}_{che} = 95$ minutes



* suppose $s_{civ} = 15$ minutes and $s_{che} = 30$ minutes



* What does this actually look like?

]

.panel[.panel-name[Data Code]

.pull-left[
```{r}
civ_min <- rnorm(n = 50, mean = 80, sd = 15)
che_min <- rnorm(n = 50, mean = 95, sd = 30)

discipline <- rep(c("civ", "che"), each = 50)
minutes <- c(civ_min, che_min)

min_data <- tibble(discipline = discipline, minutes = minutes)


```

]

.pull-right[
```{r}
# Check to make sure the dataframe looks correct
head(min_data)

```

]

]

.panel[.panel-name[Levene]
Use `leveneTest()` in the car package.

```{r}

leveneTest(minutes ~ discipline, data = min_data)

```

]

.panel[.panel-name[Plot]

.pull-left[

```{r var-hist, out.height='200px', out.width = '200px', fig.show = 'hide'}
min_data %>% 
  ggplot(aes(x = discipline, y = minutes)) + 
  geom_boxplot()

```


]

.pull-right[

```{r ref.label = 'var-hist', echo = FALSE}

```

]

]

]



---

# Independence

--
* IID is an acronym independent and identically distributed - this is a fundamental assumption that comes up a lot

--

* The results from one sample do not affect the results from another sample

--

* Ex: when students cheat on a test, that affects the independence assumption because one student’s answers influences another students’ answers


---

# When Assumptions Fail...

* We will cover two basic options for when assumptions fail

--

* Transform the data
  * Square root transformation 
  * Logarithmic transformation

--

* Use non-parametric tests

---


class: inverse, middle, center


# Chapter Six

### Correlations


---

# What Do Correlations Do?

--

* Tell us how one variable changes when another variable changes, _on average_.

--

* Positive correlation 
  * When one variable increases, the second variable increases on average
  * Similarly, when one variable decreases, the second variable decreases on average

--

* Negative correlation
  * when one variable increases, the second variable _decreases_ on average
  * Similarly, when one variable decreases, the second variable _increases_ on average

--

* They can tell us if there is _no_ relationship between variables

---

# What Don't Correlations Do?

--

* They do not tell us about causality

--

* There is nothing about direction - which variable causes or influences the other

--

* They do not tell us about the relationship between multiple (more than two) variables

---


# Data to Covariance to Correlation Coefficients


* Start with the observation that variance is calculated with:
.center[
$Variance(s^2) = \frac{\sum(x_i - \overline{x})^2}{N - 1} = \frac{\sum(x_i - \overline{x})(x_i - \overline{x})}{N - 1}$
]

--

* What if we want to know how $x$ varies with a second variable, $y$? 

--

* In practice, we really want to know: when $x_i$ is above its average value in a sample $\overline{x}$, how does $y_i$ change? Does it also tend to be above the sample average for $y$, i.e., $\overline{y}$? This is expressed in the general formular for covariance:


.center[
$cov(x,y) = \frac{\sum(x_i - \overline{x})(y_i - \overline{y})}{N - 1}$
]

--

* Covariance can be helpful, but we will be using correlation coefficients instead of covariance. This is because covariance is an unnormalized value, which can make comparisons across different ranges of values difficult.

---

# Correlation coefficient

--

* To standardize the covariance to a value that is easier to work with across ranges of values, we use the correlatoin coefficient. 

--

* There are several version of this, depending on the type of data you are working with. The most basic version is the Pearson correlation coefficient. It is calculated by dividing the covariance by the standard deviations of your two variables of interest:

.center[
$r = \frac{cov_{xy}}{s_xs_y} = \frac{\sum(x_i - \overline{x})(y_i - \overline{y})}{(N-1)s_xs_y}$
]

--

* This is a _bivariate_ correlation coefficient because it is looking at the correlation between _two variables_. There are also partial correlation coefficients, which look at the correlation between two variables whil controlling for other variables. 

--

* We can calculate the correlatoin between two variables using the `cor()` or `cor.test()` functions, which are part of base R. 


---

# What Do Correlation Coefficients Signify?

* Relationship between variables

--

* How spread out or close together the points align on a scatterplot 
  * Strong relationship - points are close to a diagonal line
  * Weak relationship - points are spread out


---

# Why Are Confidence Intervals Here?

--

* Null hypothesis: no relationship between variables, r=0

--

* $p$ value tells you if that happens

--

* For $r$ = -.3, you might see a confidence interval like (-0.25, 0.35) - the interval is symmetric about our $r$ correlation coefficient - 95% of the time, that interval will contain the true value for the population




---

class: center, middle

# Kinds of Correlation Coefficients


---

# Pearson Correlation Coefficient

--

* Use if you have:
  * Large (> 30) sample size
  * Normally distributed data
  * Interval data

--

* Values for $r$ lie in [-1,1]

--

* Rules of thumb for interpretation:
  * very small effect size: 0 < $\mid r\mid$ < 0.1 
  * small effect size: 0.1 < $\mid r\mid$ < 0.3 
  * medium effect size: 0.3 < $\mid r\mid$ < 0.5 
  * large effect size: 0.5 < $\mid r\mid$ < 1

---

# Spearman's Rho and Kendall's Tau

.pull-left[


### Spearmean's Rho
* Use when data are not normally distributed
* Specify with `cor(method="spearman")`
]


.pull-right[

### Kendall's Tau
* Use with small sample size
* Use when lots of tied ranks
* Specify with `cor(method="kendall")`

]

---

# Additional Options Not Discussed in Class

* Point Biserial Correlation

--

* Partial Correlation?





---

class: center, middle, inverse

# Start working in R


---








